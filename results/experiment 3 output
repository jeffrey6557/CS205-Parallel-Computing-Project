

# Experiment 3: Chang's modificaiton to DownPour
Each model replica fetches parameters and UPDATES them asynchronously in its own memory (splitting up all the batches into 64 processes), and pushing the final weight and loss back to master; master simply replaces its old weight 
(this is different model than Greyson's modification, where each model replica fetches weight and returns gradient = mean(w_k - w_0) (approximates true gradient by mean value theorem); Master server updates it using w -= gradient * learning rate )



# model setting are the same as in experiment except the hidden units = (24,12)

# data seting 
data = augmented data with 1000 bootstrapped samples of financial data in 2016, n_rows = N * 1000 bootstrapped where N and 

split data into 64 batches versus processing them sequentially

# testing speedups of computing updates for a model replica on 64 cores 
18.0086438656 seconds for multiprocessing
47.9421029091 seconds for sequential


# training loss history (first 64 losses are multiprocessing ) - comments we see consistent but slow convergence because the batches are bootstrapped, changing our velocity (gradient) in the parameter space somewhat inconsistently, will need to use real data for this to work
4.25236654282
4.02093839645
4.06210374832
4.00127124786
4.0480222702
4.14757013321
4.12910079956
3.98699474335
4.06528329849
3.89334559441
4.06846761703
4.09769392014
3.95776677132
4.13967657089
4.10906744003
4.1431427002
4.05278873444
4.17639350891
4.19668102264
4.14694261551
4.23916053772
4.11168861389
4.01196146011
4.02719831467
4.08887004852
3.9197845459
4.00899362564
4.1178150177
4.0246720314
4.12790250778
4.09347486496
3.96685266495
3.96011161804
4.08595180511
3.92323327065
4.0269985199
4.11925840378
4.12754535675
3.92646503448
4.07733440399
3.9652762413
4.00339841843
4.02290439606
4.2483754158
4.09744691849
4.20861721039
4.22063350677
4.05130910873
4.15609359741
4.22678089142
4.11540317535
4.05073738098
4.09247255325
3.98984646797
4.34759235382
3.80541133881
3.98418545723
4.0613155365
4.09033250809
4.14774227142
4.04519224167
4.02895879745
4.05694389343
4.20339155197
4.25236654282
3.7776324749
3.42055487633
3.08310985565
2.87075710297
2.79929542542
2.5493581295
2.35439705849
2.21644592285
2.07003092766
1.95089030266
1.87876451015
1.77890706062
1.76909911633
1.64540576935
1.61474633217
1.57017731667
1.65404975414
1.57066941261
1.53250610828
1.54822194576
1.39718472958
1.42527401447
1.42968201637
1.39097511768
1.34026539326
1.34880316257
1.32211768627
1.24251759052
1.29166471958
1.27467095852
1.29878211021
1.25252449512
1.27707874775
1.18055534363
1.22080671787
1.15042364597
1.28643405437
1.17121958733
1.17259216309
1.16763103008
1.16495394707
1.08431971073
1.2360291481
1.27019476891
1.21212530136
1.15338420868
1.14222991467
1.13195574284
1.14537155628
1.2117228508
1.18889033794
1.25249958038
1.12179505825
1.12382268906
1.07885968685
1.13376402855
1.17098462582
1.08949899673
1.17861557007
1.08943402767
1.1356164217
1.19689643383
1.22472393513

