{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cython \n",
    "#--annotate\n",
    "\n",
    "from cython.parallel import prange\n",
    "from cython cimport boundscheck, wraparound\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "import math\n",
    "import scipy.io as sio\n",
    "import time\n",
    "from libc.stdlib cimport malloc, free\n",
    "from cpython cimport array\n",
    "from cython.view cimport array as cvarray\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:,:] addMatrix(double[:,:] matA, double[:,:] matB, double[:,:] matC) nogil:\n",
    "    '''matC = matA + matB'''\n",
    "    cdef int nrow = matA.shape[0]\n",
    "    cdef int ncol = matA.shape[1]\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            matC[i,j] = matA[i,j] + matB[i,j]\n",
    "    return matC\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:,:] dotMatrix(double[:,:] matA, double[:,:] matB, double[:,:] matC) nogil:\n",
    "    '''matC = matA %*% matB, need to initialize matC as 0s'''\n",
    "    cdef int x = matA.shape[0]\n",
    "    cdef int y = matA.shape[1]\n",
    "    cdef int z = matB.shape[1]\n",
    "    for i in range(x):\n",
    "        for j in range(z):\n",
    "            matC[i,j] = 0\n",
    "    for i in range(x):\n",
    "        for j in range(y):\n",
    "            for k in range(z): \n",
    "                matC[i,k] += matA[i,j] * matB[j,k]\n",
    "    return matC\n",
    "\n",
    "# @boundscheck(False)\n",
    "# @wraparound(False)\n",
    "# cpdef double[:] matVec(double[:,:] matA, double[:] vecB, double[:] matC) nogil:\n",
    "#     '''matC = matA %*% vecB, need to initialize vecC as 0s'''\n",
    "#     cdef int x = matA.shape[0]\n",
    "#     cdef int y = matA.shape[1]\n",
    "    \n",
    "#     for i in range(x):\n",
    "#             for j in range(y):\n",
    "#                 matC[i] += matA[i,j] * vecB[j]\n",
    "#     return matC\n",
    "\n",
    "# @boundscheck(False)\n",
    "# @wraparound(False)\n",
    "# cpdef double[:,:] elemMatVec(double[:,:] matA, double[:,:] vecB, double[:,:] matC) nogil:\n",
    "#     '''element-wise matrix vector multiplication: matC[i,j] = matA[i,j] * vecB[i,0]'''\n",
    "#     cdef int x = matA.shape[0]\n",
    "#     cdef int y = matA.shape[1]\n",
    "#     cdef int z = vecB.shape[0]\n",
    "#     for i in range(x):\n",
    "#         for j in range(y):\n",
    "#             matC[i,j] = matA[i,j] * vecB[i,0]\n",
    "#     return matC\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:,:] elemMatMult(double[:,:] matA, double[:,:] matB, double[:,:] matC) nogil:\n",
    "    '''element-wise matrix multiplication: matC[i,j] = matA[i,j] * matB[i,j]'''\n",
    "    cdef int x = matA.shape[0]\n",
    "    cdef int y = matA.shape[1]\n",
    "    for i in range(x):\n",
    "        for j in range(y):\n",
    "            matC[i,j] = matA[i,j] * matB[i,j]\n",
    "    return matC\n",
    "\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:,:] addBias(double[:,:] matA, double[:,:] matB) nogil:\n",
    "    '''matA[i,j] = matA[i,j] + vecB[j,0], same row number'''\n",
    "    cdef int nrow_a = matA.shape[0]\n",
    "    cdef int ncol_a = matA.shape[1]\n",
    "    cdef int nrow_b = matB.shape[0]\n",
    "    if ncol_a == nrow_b:\n",
    "        for i in range(nrow_a):\n",
    "            for j in range(ncol_a):\n",
    "                matA[i,j] = matA[i,j] + matB[j,0]\n",
    "        return matA\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:,:] linear_addbias(double[:,:] z, double[:,:] w, double[:,:] b, double[:,:] l_out) nogil:\n",
    "    '''l_out = dot(z, w) + b'''\n",
    "    return addBias(dotMatrix(z,w,l_out),b)\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:,:] linear(double[:,:] z, double[:,:] w, double[:,:] l_out) nogil:\n",
    "    '''l_out = dot(z, w)'''\n",
    "    return dotMatrix(z,w,l_out)\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:,:] relu(double[:,:] z, double[:,:] r) nogil:\n",
    "    '''r = max(z, 0) element-wise'''\n",
    "    cdef int z_row = z.shape[0]\n",
    "    cdef int z_col = z.shape[1]\n",
    "    for i in range(z_row):\n",
    "        for j in range(z_col):\n",
    "            if z[i,j] > 0:\n",
    "                r[i,j] = z[i,j]\n",
    "            else:\n",
    "                r[i,j] = 0\n",
    "    return r\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double get_loss(double[:,:] a, double[:,:] y) nogil:\n",
    "    '''compute SE loss: loss = sum_i((a_i-y_i)^2)'''\n",
    "    # return np.sum(np.square(a-y))\n",
    "    cdef int a_len = a.shape[0]\n",
    "    cdef double loss= 0\n",
    "    for i in range(a_len):\n",
    "            loss = loss + (a[i,0]-y[i,0])**2\n",
    "    return loss\n",
    "\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:,:] lossGradient(double[:,:] a, double[:,:] y, double[:,:] loss_grad) nogil:\n",
    "    '''gradient of SE loss: loss_grad = 2*(a-y)'''\n",
    "    cdef int a_len = a.shape[0]\n",
    "    for i in range(a_len):\n",
    "            loss_grad[i,0] = 2*(a[i,0]-y[i,0])\n",
    "    return loss_grad # gradient of loss with respect to predicted outputs \n",
    "\n",
    "\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:,:] reluGradient(double[:,:] z, double[:,:] relu_grad) nogil:\n",
    "    '''gradient of relu: relu_grad = z>0'''\n",
    "    cdef int i,j, z_row = z.shape[0], z_col = z.shape[1]\n",
    "    for i in range(z_row):\n",
    "        for j in range(z_col):\n",
    "            if z[ i,j] > 0:\n",
    "                relu_grad[i,j] = 1\n",
    "            else:\n",
    "                relu_grad[i,j] = 0\n",
    "    return relu_grad\n",
    "\n",
    "\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:,:] forward_prop_naive(double[:,:] theta_1, double[:,:] theta_2, double[:] layer_structure_cumsum,\n",
    "                double[:,:] inputs, double[:,:] labels, double[:,:] predictions, \n",
    "                double[:,:] layer_weighted_ave_1, double[:,:] layer_weighted_ave_2,\n",
    "                double[:,:] layer_output_2\n",
    "                )nogil:\n",
    "    '''forward propagation: make predictions\n",
    "       Parameters: theta, weights; bs, intercepts; \n",
    "                   layer_structure_cumsum, cumulative sum of number of neurons in each layer;\n",
    "                   inputs, n by m matrix, where n is the number of time stamps; labels, the truth;\n",
    "                   predictions, empty vector (place holder), to be updated and returned;\n",
    "                   place holders for layer_weighted_ave_1,2 and layer_output_1;\n",
    "       Outpus: predictions, the predicted value (output of ANN); layer_output_1\n",
    "    '''\n",
    "    cdef int n_inputs = inputs.shape[0]\n",
    "    cdef int n_layers = int(layer_structure_cumsum.shape[0])\n",
    "    cdef int n_neurons_1 = int(layer_structure_cumsum[0])#input layer\n",
    "    cdef int n_neurons_2 = int(layer_structure_cumsum[1] - layer_structure_cumsum[0])#1st hidden layer\n",
    "    cdef int n_neurons_3 = int(layer_structure_cumsum[2] - layer_structure_cumsum[1])#output layer\n",
    "    # input layer to first hidden layer\n",
    "    layer_weighted_ave_1[:,:] = linear(inputs, theta_1, layer_weighted_ave_1) # n x m, m x n_neurons_2 => n x n_neurons_2\n",
    "    layer_output_2[:,:] = relu(layer_weighted_ave_1, layer_output_2) #n x n_neurons_2, outputs of layer 2\n",
    "    # first hidden layer to output layer\n",
    "    layer_weighted_ave_2[:,:] = linear(layer_output_2, theta_2, layer_weighted_ave_2) # n x n_neurons_2, n_neurons_2 x output_dim => n x output_dim  \n",
    "    predictions[:,:] = layer_weighted_ave_2 # assuming output_dim = 1.\n",
    "    return predictions\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double compute_loss_for_mpi(inputs, labels, weights, layer_structure):\n",
    "    '''compute loss; function called by mpi; with gil (can use numpy)\n",
    "       Parameters: theta, weights; bs, intercepts; \n",
    "                   layer_structure_cumsum, cumulative sum of number of neurons in each layer;\n",
    "                   inputs, n by m matrix, where n is the number of time stamps; labels, the truth;\n",
    "       Outpus: loss\n",
    "    '''\n",
    "    cdef int n_inputs = inputs.shape[0]\n",
    "    theta_1 = weights[0]\n",
    "    theta_2 = weights[1]\n",
    "    # initialize all matrices necessary for forward_prop_naive; \n",
    "    # need to pass place holders because hard to build arrays with no gil.\n",
    "    layer_structure_cumsum = np.cumsum(layer_structure)\n",
    "    cdef int n_neurons_1 = int(layer_structure_cumsum[0])#input layer\n",
    "    cdef int n_neurons_2 = int(layer_structure_cumsum[1] - layer_structure_cumsum[0])#1st hidden layer\n",
    "    cdef int n_neurons_3 = int(layer_structure_cumsum[2] - layer_structure_cumsum[1])#output layer\n",
    "    cdef double[:,:] layer_weighted_ave_1 = np.zeros((n_inputs, n_neurons_2)) #n x n_neurons_2 \n",
    "    cdef double[:,:] layer_weighted_ave_2 = np.zeros((n_inputs, n_neurons_3)) #n x n_neurons_3 = n x output_dim\n",
    "    cdef double[:,:] layer_output_2 = np.zeros((n_inputs, n_neurons_2)) #same as layer_weighted_ave_1\n",
    "    cdef double[:,:] predictions = np.zeros((n_inputs,1))\n",
    "    predictions = forward_prop_naive(theta_1, theta_2, layer_structure_cumsum, inputs, labels, predictions, \n",
    "                layer_weighted_ave_1, layer_weighted_ave_2, layer_output_2)\n",
    "    cdef double loss = get_loss(predictions, labels)/n_inputs\n",
    "    return loss\n",
    "\n",
    "def test_mpi_api():\n",
    "    n = 100\n",
    "    m = 4\n",
    "    # layer_structure_cumsum = np.cumsum(np.array([m, int(m/2), 1]), dtype='int')\n",
    "    # layer_structure_cumsum = np.array([m, m+int(m/2), m+int(m/2)+1], dtype='i')\n",
    "    layer_structure = np.array([m, np.floor(m/2), 1])\n",
    "    print layer_structure\n",
    "    cdef double[:,:] inputs_raw = np.random.randn(n, m)\n",
    "    cdef double[:,:] inputs = np.c_[np.ones(n), inputs_raw] \n",
    "    cdef double[:,:] labels = (np.dot(inputs, np.array([0.1,1,2,3,4]))).reshape(-1,1)\n",
    "    cdef double[:,:] theta_1 = np.random.randn(m, int(m/2))\n",
    "    cdef double[:,:] theta_2 = np.random.randn(int(m/2), 1)\n",
    "    weights = [theta_1, theta_2]\n",
    "    loss = compute_loss_for_mpi(inputs, labels, weights, layer_structure)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:] train_epoch(double[:,:] theta_1, double[:,:] theta_2, double[:] layer_structure_cumsum,\n",
    "                double[:,:] inputs, double[:,:] labels, double[:,:] predictions,\n",
    "                double[:,:] layer_weighted_ave_1, double[:,:] layer_weighted_ave_2,\n",
    "                double[:,:] layer_output_2, double[:,:] layer_output_2_grad, \n",
    "                double[:,:] theta_grad_1, double[:,:] theta_grad_2,\n",
    "                double[:,:] delta_2, double[:,:] delta_3, double[:] theta_grad_all\n",
    "                )nogil:\n",
    "    '''forward propagation: make predictions\n",
    "       Parameters: theta_1,2, weights; bs, intercepts; \n",
    "                   layer_structure_cumsum, cumulative sum of number of neurons in each layer;\n",
    "                   inputs, n by m matrix, where n is the number of time stamps; labels, the truth;\n",
    "                   predictions, empty vector (place holder), to be updated and returned;\n",
    "                   place holders for layer_weighted_ave_1,2, layer_output_2, \n",
    "                   theta_grad_1,2(p x m, m x 1), delta_2,3(n x m, n x 1);\n",
    "       Outpus: predictions, the predicted value (output of ANN)\n",
    "    '''\n",
    "    # forwar prop\n",
    "    cdef int n_inputs = inputs.shape[0]\n",
    "    cdef int m_inputs = inputs.shape[1]\n",
    "    cdef int n_layers = int(layer_structure_cumsum.shape[0])\n",
    "    cdef int n_neurons_1 = int(layer_structure_cumsum[0])#input layer\n",
    "    cdef int n_neurons_2 = int(layer_structure_cumsum[1] - layer_structure_cumsum[0])#1st hidden layer\n",
    "    cdef int n_neurons_3 = int(layer_structure_cumsum[2] - layer_structure_cumsum[1])#output layer\n",
    "    # input layer to first hidden layer\n",
    "    layer_weighted_ave_1[:,:] = linear(inputs, theta_1, layer_weighted_ave_1) # n x m, m x n_neurons_2 => n x n_neurons_2\n",
    "#     print 'layer_weighted_ave_1',np.asarray(layer_weighted_ave_1)\n",
    "    layer_output_2[:,:] = relu(layer_weighted_ave_1, layer_output_2) #n x n_neurons_2, outputs of layer 2\n",
    "#     print 'layer_output_2_after',np.asarray(layer_output_2)\n",
    "    # first hidden layer to output layer\n",
    "#     print 'layer_weighted_ave_2',np.asarray(layer_weighted_ave_2)\n",
    "    layer_weighted_ave_2[:,:] = linear(layer_output_2, theta_2, layer_weighted_ave_2) # n x n_neurons_2, n_neurons_2 x output_dim => n x output_dim  \n",
    "    predictions[:,:] = layer_weighted_ave_2 # assuming output_dim = 1.\n",
    "#     print 'layer_weighted_ave_2_after',np.asarray(layer_weighted_ave_2)\n",
    "#     print 'theta_2',np.asarray(theta_2)\n",
    "#     print 'losssssssssssss',get_loss(predictions, labels)/n_inputs\n",
    "    \n",
    "\n",
    "    # back propagation: calculate gradiants\n",
    "#     print '*********************************delta_2************************************************************'\n",
    "#     print 'delta3', np.array(delta_3)\n",
    "    delta_3[:,:] = lossGradient(predictions, labels, delta_3)  # n x output_dim\n",
    "#     print '*****predictions',np.array(predictions)\n",
    "#     print '******labels',np.array(labels)\n",
    "#     print 'delta3_after',np.asarray(delta_3)\n",
    "    # for i in range(n_inputs):\n",
    "        # assume delta2[i,:] is output1[i,:] is 1 x m, theta2 is m x 1, delta3[i] is 1 x 1\n",
    "        # delta2[i,:] = np.dot(np.dot(np.diagflat(output1[i,:]),theta2),delta3[i])\n",
    "        #  1 x m =  1 x m , m x 1, 1 x1 \n",
    "    # delta_2[:,:] = elemMatVec(dotMatrix(layer_output_2, theta_2, delta_2), delta_3, delta_2) # n x 1\n",
    "#     print '*********************************delta_2************************************************************'\n",
    "#     print 'delta_2',np.asarray(delta_2)\n",
    "    layer_output_2_grad[:,:] = reluGradient(layer_output_2, layer_output_2_grad)#n x n_neurons_2\n",
    "    delta_2[:,:] = dotMatrix(theta_2, delta_3.T, delta_2.T).T\n",
    "    delta_2[:,:] = elemMatMult(layer_output_2_grad, delta_2, delta_2) # (n_neurons_2 x 1, 1 x n).T\n",
    "#     print 'delta_2_after',np.asarray(delta_2)\n",
    "#     print 'layer_output_2', np.asarray(layer_output_2)\n",
    "#     print 'layer_output_2_grad',np.asarray(layer_output_2_grad)\n",
    "#     print 'theta_2',np.asarray(theta_2)\n",
    "#     print 'delta3.T', np.asarray(delta_3.T)\n",
    "    \n",
    "    # input to first hidden layer: m_inputs x n_neurons_2     =  m_inputs x n_inputs , n_inputs x n_neurons_2\n",
    "#     print '*********************************theta_grad_1************************************************************'\n",
    "#     print 'theta_grad_1_before',np.asarray(theta_grad_1)\n",
    "    theta_grad_1[:,:] = dotMatrix(inputs.T, delta_2, theta_grad_1) # m_inputs x n_neurons_2\n",
    "#     print 'theta_grad_1_after',np.asarray(theta_grad_1)\n",
    "#     print 'delta_2',np.asarray(delta_2)\n",
    "    # first hidden layer to output: n_neurons_2 x output_dim    = n_neurons_2 x n , n x output_dim\n",
    "#     print '*********************************theta_grad_2************************************************************'\n",
    "#     print 'theta_grad_2',np.asarray(theta_grad_2)\n",
    "    theta_grad_2[:,:] = dotMatrix(layer_output_2.T, delta_3, theta_grad_2) # n_neurons_2 x output_dim (=1)\n",
    "#     print 'theta_grad_2_after',np.asarray(theta_grad_2)\n",
    "#     print 'layer_output_2.T',np.asarray(layer_output_2.T)\n",
    "#     print 'delta_3',np.asarray(delta_3)\n",
    "#     print '*********************************grad_all************************************************************'\n",
    "    theta_grad_all[:] = combine_grads(theta_grad_1, theta_grad_2, theta_grad_all)\n",
    "#     print np.asarray(theta_grad_all)\n",
    "    \n",
    "    \n",
    "    return theta_grad_all\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double[:] combine_grads(double[:,:] matA, double[:,:] matB, double[:] C) nogil:\n",
    "    cdef int nrow_A = matA.shape[0]\n",
    "    cdef int ncol_A = matA.shape[1]\n",
    "    cdef int nrow_B = matB.shape[0]\n",
    "    cdef int ncol_B = matB.shape[1]\n",
    "    cdef int i, j\n",
    "    for i in range(nrow_A):\n",
    "        for j in range(ncol_A):\n",
    "            C[i*ncol_A+j] = matA[i,j]\n",
    "    for i in range(nrow_B):\n",
    "        for j in range(ncol_B): \n",
    "            C[nrow_A*ncol_A+i*ncol_B+j] = matB[i,j]\n",
    "    return C\n",
    "\n",
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "def para_train(inputs, labels, weights, layer_structure):\n",
    "    N_THREADS = 2\n",
    "    B_SIZE = 10\n",
    "    #add intercept column\n",
    "    n_inputs_all = inputs.shape[0]\n",
    "    n_inputs = n_inputs_all\n",
    "    cdef double[:] layer_structure_cumsum = np.cumsum(layer_structure)\n",
    "    cdef int n_neurons_1 = int(layer_structure_cumsum[0])#input layer\n",
    "    cdef int n_neurons_2 = int(layer_structure_cumsum[1] - layer_structure_cumsum[0])#1st hidden layer\n",
    "    cdef int n_neurons_3 = int(layer_structure_cumsum[2] - layer_structure_cumsum[1])#output layer\n",
    "    cdef double[:,:] theta_1 = weights[0]\n",
    "    cdef double[:,:] theta_2 = weights[1]\n",
    "    cdef double[:,:] predictions = np.zeros((n_inputs,1))\n",
    "    cdef double[:,:] layer_weighted_ave_1 = np.zeros((n_inputs, n_neurons_2))\n",
    "    cdef double[:,:] layer_weighted_ave_2 = np.zeros((n_inputs, n_neurons_3))\n",
    "    cdef double[:,:] layer_output_2 = np.zeros((n_inputs, n_neurons_2))\n",
    "    cdef double[:,:] layer_output_2_grad = np.zeros((n_inputs, n_neurons_2))\n",
    "    cdef double[:,:] theta_grad_1 = np.zeros_like(theta_1)\n",
    "    cdef double[:,:] theta_grad_2 = np.zeros_like(theta_2)\n",
    "    cdef double[:,:] delta_2 = np.zeros_like(layer_weighted_ave_1)\n",
    "    cdef double[:,:] delta_3 = np.zeros_like(layer_weighted_ave_2)\n",
    "    #p_iters = int(n_inputs/(B_SIZE*N_THREADS))\n",
    "    p_iters = 30\n",
    "    \n",
    "    cdef double[:] theta_grad_all = np.zeros(theta_1.shape[0]*theta_1.shape[1]+theta_2.shape[0]*theta_2.shape[1])\n",
    "    cdef double[:,:] theta_grad_sum = np.zeros((p_iters,theta_grad_all.shape[0]))\n",
    "    print theta_grad_all.shape[0]\n",
    "    #cdef double[:,:] theta_grad_all_mat = np.zeros((5, ))\n",
    "    cdef int i\n",
    "    cdef int j\n",
    "    cdef double[:] loss = np.zeros(p_iters)\n",
    "    cdef int ip,k\n",
    "    for ip in prange(1, num_threads = 1, nogil=True):\n",
    "        for i in range(p_iters):\n",
    "            theta_grad_all[:] = train_epoch(theta_1, theta_2, layer_structure_cumsum,\n",
    "                    inputs, labels, predictions, layer_weighted_ave_1, layer_weighted_ave_2, layer_output_2, \n",
    "                    layer_output_2_grad, theta_grad_1, theta_grad_2, delta_2, delta_3, theta_grad_all)\n",
    "            for j in range(theta_grad_all.shape[0]):\n",
    "                theta_grad_sum[i,j] = theta_grad_all[j]\n",
    "            \n",
    "            for j in range(theta_1.shape[0]):\n",
    "                for k in range(theta_1.shape[1]):\n",
    "                    theta_1[j,k] = theta_1[j,k] - 0.000001* theta_grad_1[j,k]\n",
    "            for j in range(theta_1.shape[0]):\n",
    "                for k in range(theta_1.shape[1]):\n",
    "                    theta_2[j,k] = theta_2[j,k] - 0.000001* theta_grad_2[j,k]\n",
    "            predictions[:] = forward_prop_naive(theta_1, theta_2, layer_structure_cumsum, inputs, labels, predictions, \n",
    "                layer_weighted_ave_1, layer_weighted_ave_2, layer_output_2)\n",
    "            loss[i] = get_loss(predictions, labels)\n",
    "    return theta_grad_sum, loss\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.  2.  1.]\n",
      "28.8912141544\n"
     ]
    }
   ],
   "source": [
    "print test_mpi_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_para_train():\n",
    "    n = 100\n",
    "    m = 4\n",
    "    # layer_structure_cumsum = np.cumsum(np.array([m, int(m/2), 1]), dtype='int')\n",
    "    # layer_structure_cumsum = np.array([m, m+int(m/2), m+int(m/2)+1], dtype='i')\n",
    "    layer_structure = np.array([m+1, np.floor(m/2), 1])\n",
    "    print layer_structure\n",
    "    inputs_raw = np.random.randn(n, m)+2\n",
    "    inputs = np.c_[np.ones(n), inputs_raw] \n",
    "    labels = (np.dot(inputs, np.array([0.1,1,2,3,4]))).reshape(-1,1)\n",
    "    theta_1 = np.random.randn(m+1, int(m/2))+5\n",
    "    theta_2 = np.random.randn(int(m/2), 1)+5\n",
    "    weights = [theta_1, theta_2]\n",
    "    grads = para_train(inputs, labels, weights, layer_structure)\n",
    "    return grads\n",
    "theta_grad_sum, loss=test_para_train()\n",
    "# print np.asarray(test_para_train())\n",
    "# print test_mpi_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@boundscheck(False)\n",
    "@wraparound(False)\n",
    "cpdef double compute_loss_for_mpi(double[:,::1] theta_1, double[:,::1] theta_2, \n",
    "                                  double[::1] bs, int[::1] layer_structure_cumsum,\n",
    "                                  double[:,::1] inputs, double[::1] labels):\n",
    "    '''compute loss; function called by mpi; with gil (can use numpy)\n",
    "       Parameters: theta, weights; bs, intercepts; \n",
    "                   layer_structure_cumsum, cumulative sum of number of neurons in each layer;\n",
    "                   inputs, n by m matrix, where n is the number of time stamps; labels, the truth;\n",
    "       Outpus: loss\n",
    "    '''\n",
    "    \n",
    "    cdef int n_inputs = inputs.shape[0]\n",
    "    # initialize all matrices necessary for forward_prop_naive; \n",
    "    # need to pass place holders because hard to build arrays with no gil.\n",
    "    cdef int n_neurons_1 = layer_structure_cumsum[0]\n",
    "    cdef double[:,:] layer_weighted_ave_1 = np.zeros((n_inputs, n_neurons_1)) #n x n_neurons_1 \n",
    "    cdef int n_neurons_2 = layer_structure_cumsum[1] - layer_structure_cumsum[0]\n",
    "    cdef double[:,:] layer_weighted_ave_2 = np.zeros((n_inputs, n_neurons_2)) #n x n_neurons_2 = n x output_dim\n",
    "    cdef double[:,:] layer_output_1 = np.zeros((n_inputs, n_neurons_2)) #same as layer_weighted_ave_1\n",
    "    cdef double[:] predictions = np.zeros((n_inputs,))\n",
    "    predictions,_ = forward_prop_naive(theta_1, theta_2, bs, layer_structure_cumsum, inputs, labels, predictions, \n",
    "                layer_weighted_ave_1, layer_weighted_ave_2, layer_output_1)\n",
    "    # print np.asarray(predictions)\n",
    "    cdef double loss = get_loss(predictions, labels)/n_inputs\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6, 7])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 100\n",
    "m = 4\n",
    "# layer_structure_cumsum = np.cumsum(np.array([m, int(m/2), 1]), dtype='int')\n",
    "# layer_structure_cumsum = np.array([m, m+int(m/2), m+int(m/2)+1], dtype='i')\n",
    "layer_structure = np.array([m, int(m//2), 1], dtype=np.int64)\n",
    "np.cumsum(layer_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Error compiling Cython file:\n",
      "------------------------------------------------------------\n",
      "...\n",
      "    for i in range(inputs_nrow):\n",
      "        \n",
      "        # assume delta2[i,:] is output1[i,:] is 1 x m, theta2 is m x 1, delta3[i] is 1 x 1\n",
      "        # delta2[i,:] = np.dot(np.dot(np.diagflat(output1[i,:]),theta2),delta3[i])\n",
      "        #  1 x m =  1 x m , m x 1, 1 x1 \n",
      "        delta2[i,:] = dotMatrix(dotMatrix(output1[i,:].reshape((1,-1))),theta2),delta3[i])\n",
      "                                                                                        ^\n",
      "------------------------------------------------------------\n",
      "\n",
      "/Users/linglinhuang/.ipython/cython/_cython_magic_6bfa9c686dabdf93da0fda916568a71c.pyx:202:89: Syntax error in simple statement list\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# @boundscheck(False)\n",
    "# @wraparound(False)\n",
    "# cpdef double[:] forward_prop(double[:,:] theta, double[:] bs, int[:] layer_structure_cumsum,\n",
    "#                    double[:,:] inputs, double[:] labels, double[:] predictions) nogil:\n",
    "#     '''forward propagation: make predictions\n",
    "#        Parameters: theta, weights; bs, intercepts; \n",
    "#                    layer_structure_cumsum, cumulative sum of number of neurons in each layer;\n",
    "#                    inputs, n by m matrix, where n is the number of time stamps; labels, the truth;\n",
    "#                    predictions, empty vector (place holder), to be updated and returned\n",
    "#        Outpus: predictions, the predicted value (output of ANN)\n",
    "#     '''\n",
    "#     cdef int n_inputs = inputs.shape[0]\n",
    "#     cdef int n_layers = layer_structure_cumsum.shape[0]\n",
    "#     cdef double[:,:] layer_input = inputs\n",
    "#     cdef double[:,:] layer_output = inputs\n",
    "#     cdef double[:,:] layer_activated = inputs\n",
    "#     cdef double[:] layer_bias = labels\n",
    "#     cdef double[:,:] layer_weights = inputs\n",
    "    \n",
    "#     for layer in range(n_layers):\n",
    "#         # compute inputs for this layer\n",
    "#         if layer == 0:#input layer\n",
    "#             layer_input[:,:] = inputs\n",
    "#             layer_bias[:] = bs[0:layer_structure_cumsum[layer]]\n",
    "#             layer_weights[:] = theta[:,0:layer_structure_cumsum[layer]]\n",
    "#         else: #\n",
    "#             layer_input[:,:] = layer_output\n",
    "#             layer_bias[:] = bs[layer_structure_cumsum[layer-1]:layer_structure_cumsum[layer]]\n",
    "#             layer_weights[:] = theta[:,layer_structure_cumsum[layer-1]:layer_structure_cumsum[layer]]\n",
    "#         # activation function\n",
    "        \n",
    "#         layer_activated[:] = linear(layer_input, layer_weights, layer_bias, layer_output)\n",
    "#         # compute outputs for this layer\n",
    "#         if layer == n_layers-1:# the output layer\n",
    "#             predictions[:] = layer_activated\n",
    "#         else:\n",
    "#             layer_output[:] = relu(layer_activated, layer_output)\n",
    "    \n",
    "#     return predictions\n",
    "    \n",
    "#######################\n",
    "# cpdef test():\n",
    "#     cdef int nrow = 3\n",
    "#     cdef int ncol = 3\n",
    "#     cdef double[:,:] A = np.random.randn(nrow, ncol)\n",
    "#     cdef double[:,:] B = np.ones((nrow, ncol))\n",
    "#     cdef double[:,:] C = np.zeros((nrow, ncol))\n",
    "#     cdef double[:] bias = np.ones((nrow,))\n",
    "#     cdef double[:] labels = np.zeros((nrow,))\n",
    "#     cdef double[:] gradloss = np.zeros((nrow,))\n",
    "#     cdef int[:] layer_structure_cumsum = np.array((3,8,1), dtype='i')\n",
    "#     cdef double loss = 0\n",
    "#     cdef double[:] predictions  = np.array(nrow)\n",
    "    \n",
    "\n",
    "#     cdef int x = 0\n",
    "#     with nogil:\n",
    "#         for x in prange(3):        \n",
    "#             pass\n",
    "# #             loss = compute_loss_for_mpi(B, bias, layer_structure_cumsum, B, labels\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
